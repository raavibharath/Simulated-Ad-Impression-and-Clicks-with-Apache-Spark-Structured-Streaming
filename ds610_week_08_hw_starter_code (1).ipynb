{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cada25b-2e40-4ad5-8191-b12dd27ce61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DS-610 Week 8 Homework: Structured Streaming API in Apache Spark\n",
    "This homework is adapted from:\n",
    "https://docs.databricks.com/_extras/notebooks/source/stream-stream-joins-scala.html\n",
    "\n",
    "For this homework we will practice structured streaming API in Apache Spark with a case study on a simulated data. The data consists of simulated advertisement impressions and clicks on a mobile device. The goal here is to find a set of advertisements with a *meaningful click*. The word *meaningful* can be broadly defined as it is always hard to guess the intentions of a user on a mobile device. Some clicks are due to a mistake, and some clicks are just too far apart in time to be meaningful. For this homework we will assume that each advertisement is shown exactly once in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b96afe-b276-4d4e-ba87-7e4c975a08ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Simulated Ad Impression and Click\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afbb92f-2388-4a25-bed1-15981e0b0533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Starting Simulation Data Streams\n",
    "We will leverage `rate` source in Apache Spark to generate streams of timestamped data. The first is a time stamped data that represents the ad impressions, i.e. the pair of advertisement id and the timestamp at which the ad was displayed to the user. The second stream is a time stamped data that represents the ad clicks, i.e. the pair of advertisement id and the timestamp at which the ad was clicked by the user.\n",
    "\n",
    "The goal of computational advertisement is to maximize the chance that a user clicks on a set of advertisements that are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b750ba2-8294-4eb1-89f7-6524388fe265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "impressions = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load()\n",
    "impressions = impressions.select(impressions.value.alias(\"shownAdId\"), impressions.timestamp.alias(\"impressionTime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6a46d2-7d1b-4dbd-b024-f25bc717a1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 10 out of every 100 impressions result in a click\n",
    "clicks = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load().where((F.rand() * 100).cast(\"integer\") < 10)\n",
    "\n",
    "# -50 so that a click with same id as impression is generated much later.\n",
    "clicks = clicks.select((clicks.value - 50).alias(\"clickedAdId\"), clicks.timestamp.alias(\"clickTime\")).where(\"clickedAdId > 0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e47632-f188-4762-b850-0ec6f7c03fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "Below we will see the schema for `impressions`. The first column is ID number of the advertisement (`adId`) and the second column is the time when it was displayed on the screen (`impressionTime`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d530a266-0442-4c28-8389-870c1d93002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n|shownAdId|impressionTime         |\n+---------+-----------------------+\n|0        |2025-01-20 01:18:00.983|\n|1        |2025-01-20 01:18:01.183|\n|2        |2025-01-20 01:18:01.383|\n|3        |2025-01-20 01:18:01.583|\n|4        |2025-01-20 01:18:01.783|\n+---------+-----------------------+\n\n+---------+-----------------------+\n|shownAdId|impressionTime         |\n+---------+-----------------------+\n|0        |2025-01-20 01:18:00.983|\n|1        |2025-01-20 01:18:01.183|\n|2        |2025-01-20 01:18:01.383|\n|3        |2025-01-20 01:18:01.583|\n|4        |2025-01-20 01:18:01.783|\n|5        |2025-01-20 01:18:01.983|\n|6        |2025-01-20 01:18:02.183|\n|7        |2025-01-20 01:18:02.383|\n|8        |2025-01-20 01:18:02.583|\n|9        |2025-01-20 01:18:02.783|\n|10       |2025-01-20 01:18:02.983|\n|11       |2025-01-20 01:18:03.183|\n|12       |2025-01-20 01:18:03.383|\n|13       |2025-01-20 01:18:03.583|\n|14       |2025-01-20 01:18:03.783|\n+---------+-----------------------+\n\n+---------+-----------------------+\n|shownAdId|impressionTime         |\n+---------+-----------------------+\n|0        |2025-01-20 01:18:00.983|\n|1        |2025-01-20 01:18:01.183|\n|2        |2025-01-20 01:18:01.383|\n|3        |2025-01-20 01:18:01.583|\n|4        |2025-01-20 01:18:01.783|\n|5        |2025-01-20 01:18:01.983|\n|6        |2025-01-20 01:18:02.183|\n|7        |2025-01-20 01:18:02.383|\n|8        |2025-01-20 01:18:02.583|\n|9        |2025-01-20 01:18:02.783|\n|10       |2025-01-20 01:18:02.983|\n|11       |2025-01-20 01:18:03.183|\n|12       |2025-01-20 01:18:03.383|\n|13       |2025-01-20 01:18:03.583|\n|14       |2025-01-20 01:18:03.783|\n|15       |2025-01-20 01:18:03.983|\n|16       |2025-01-20 01:18:04.183|\n|17       |2025-01-20 01:18:04.383|\n|18       |2025-01-20 01:18:04.583|\n|19       |2025-01-20 01:18:04.783|\n|20       |2025-01-20 01:18:04.983|\n|21       |2025-01-20 01:18:05.183|\n|22       |2025-01-20 01:18:05.383|\n|23       |2025-01-20 01:18:05.583|\n|24       |2025-01-20 01:18:05.783|\n|25       |2025-01-20 01:18:05.983|\n|26       |2025-01-20 01:18:06.183|\n|27       |2025-01-20 01:18:06.383|\n|28       |2025-01-20 01:18:06.583|\n|29       |2025-01-20 01:18:06.783|\n|30       |2025-01-20 01:18:06.983|\n|31       |2025-01-20 01:18:07.183|\n|32       |2025-01-20 01:18:07.383|\n|33       |2025-01-20 01:18:07.583|\n|34       |2025-01-20 01:18:07.783|\n+---------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "impressions_query = impressions.writeStream.queryName(\"impressions\").format(\"memory\").outputMode(\"append\").start()\n",
    "non_empty_count = 0\n",
    "while non_empty_count < 3:\n",
    "    result = spark.sql(\"SELECT * FROM impressions\")\n",
    "    row_count = result.count()\n",
    "    if row_count == 0:\n",
    "        continue\n",
    "    non_empty_count += 1\n",
    "    result.show(row_count, truncate=False)\n",
    "    time.sleep(3)\n",
    "impressions_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d2fc64-b324-4565-a6cd-b9eaf61d503e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "Similarly we see the schema for `clicks` where the first column is `adId` and the second columns is the time when the advertisement was clicked (`clickTime`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db43abc0-b4cf-46f8-b147-f44c657f6e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n|clickedAdId|clickTime              |\n+-----------+-----------------------+\n|13         |2025-01-20 01:18:25.955|\n+-----------+-----------------------+\n\n+-----------+-----------------------+\n|clickedAdId|clickTime              |\n+-----------+-----------------------+\n|13         |2025-01-20 01:18:25.955|\n+-----------+-----------------------+\n\n+-----------+-----------------------+\n|clickedAdId|clickTime              |\n+-----------+-----------------------+\n|13         |2025-01-20 01:18:25.955|\n|31         |2025-01-20 01:18:29.555|\n|33         |2025-01-20 01:18:29.955|\n+-----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "clicks_query = clicks.writeStream.queryName(\"clicks\").format(\"memory\").outputMode(\"append\").start()\n",
    "non_empty_count = 0\n",
    "while non_empty_count < 3:\n",
    "    result = spark.sql(\"select * from clicks\")\n",
    "    row_count = result.count()\n",
    "    if row_count == 0:\n",
    "        continue\n",
    "    non_empty_count += 1\n",
    "    result.show(row_count, truncate=False)\n",
    "    time.sleep(3)\n",
    "clicks_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d4c0f2-f9f9-489f-9a4a-2026688d2a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Part 1: Inner Join of Streaming DataFrames\n",
    "### Part 1a\n",
    "Write a code that joins the streaming DataFrame `impressions` and the streaming DataFrame `clicks` on the column `shownAdId = clickedAdId'. Then create a `StreamingQuery` by calling `writeStream`. Use `outputMode` as `append` and `format` as `memory`. Set the queryName via `queryName` method. Then finally fire off the query by calling `start` method on it. For your reference, the Scala version of the code is given in the website:\n",
    "https://docs.databricks.com/_extras/notebooks/source/stream-stream-joins-scala.html\n",
    "\n",
    "For Python API, please refer to:\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "Part 1a requires joining two column names `shownAdId` with `clickedAdId` so we will find the following useful:\n",
    "https://stackoverflow.com/questions/43675465/how-to-pass-join-condition-as-a-parameter-to-spark-dataframe-joins\n",
    "\n",
    "**Note.** For our homework, the inner join is written to the memory but we can also leverage other sink types. For reference, please see the website links referenced in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e00df72-1dc4-4f5d-8250-3517452e157d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Complete this part for Part 1a. This function should return a StreamingQuery that is a result of an innerjoin\n",
    "# between the two streaming dataframes, impressions and clicks.\n",
    "from pyspark.sql.functions import expr\n",
    "joined_stream_name = 'inner_join_impression_click'\n",
    "\n",
    "inner_join_query = impressions.join(clicks, expr(\"shownAdId = clickedAdId\"))\\\n",
    "  .writeStream\\\n",
    "    .outputMode('append')\\\n",
    "      .format('memory')\\\n",
    "        .queryName(joined_stream_name)\\\n",
    "          .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49418322-513c-48dc-8283-87bfcfb2e1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+-----------+----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime             |\n+---------+-----------------------+-----------+----------------------+\n|29       |2025-01-20 01:18:44.045|29         |2025-01-20 01:18:54.33|\n|54       |2025-01-20 01:18:49.045|54         |2025-01-20 01:18:59.33|\n|95       |2025-01-20 01:18:57.245|95         |2025-01-20 01:19:07.53|\n|3        |2025-01-20 01:18:38.845|3          |2025-01-20 01:18:49.13|\n|37       |2025-01-20 01:18:45.645|37         |2025-01-20 01:18:55.93|\n|12       |2025-01-20 01:18:40.645|12         |2025-01-20 01:18:50.93|\n|66       |2025-01-20 01:18:51.445|66         |2025-01-20 01:19:01.73|\n|13       |2025-01-20 01:18:40.845|13         |2025-01-20 01:18:51.13|\n|36       |2025-01-20 01:18:45.445|36         |2025-01-20 01:18:55.73|\n|14       |2025-01-20 01:18:41.045|14         |2025-01-20 01:18:51.33|\n|38       |2025-01-20 01:18:45.845|38         |2025-01-20 01:18:56.13|\n|53       |2025-01-20 01:18:48.845|53         |2025-01-20 01:18:59.13|\n+---------+-----------------------+-----------+----------------------+\n\n+---------+-----------------------+-----------+----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime             |\n+---------+-----------------------+-----------+----------------------+\n|29       |2025-01-20 01:18:44.045|29         |2025-01-20 01:18:54.33|\n|54       |2025-01-20 01:18:49.045|54         |2025-01-20 01:18:59.33|\n|95       |2025-01-20 01:18:57.245|95         |2025-01-20 01:19:07.53|\n|3        |2025-01-20 01:18:38.845|3          |2025-01-20 01:18:49.13|\n|37       |2025-01-20 01:18:45.645|37         |2025-01-20 01:18:55.93|\n|12       |2025-01-20 01:18:40.645|12         |2025-01-20 01:18:50.93|\n|66       |2025-01-20 01:18:51.445|66         |2025-01-20 01:19:01.73|\n|13       |2025-01-20 01:18:40.845|13         |2025-01-20 01:18:51.13|\n|36       |2025-01-20 01:18:45.445|36         |2025-01-20 01:18:55.73|\n|14       |2025-01-20 01:18:41.045|14         |2025-01-20 01:18:51.33|\n|38       |2025-01-20 01:18:45.845|38         |2025-01-20 01:18:56.13|\n|53       |2025-01-20 01:18:48.845|53         |2025-01-20 01:18:59.13|\n+---------+-----------------------+-----------+----------------------+\n\n+---------+-----------------------+-----------+----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime             |\n+---------+-----------------------+-----------+----------------------+\n|29       |2025-01-20 01:18:44.045|29         |2025-01-20 01:18:54.33|\n|54       |2025-01-20 01:18:49.045|54         |2025-01-20 01:18:59.33|\n|95       |2025-01-20 01:18:57.245|95         |2025-01-20 01:19:07.53|\n|3        |2025-01-20 01:18:38.845|3          |2025-01-20 01:18:49.13|\n|37       |2025-01-20 01:18:45.645|37         |2025-01-20 01:18:55.93|\n|12       |2025-01-20 01:18:40.645|12         |2025-01-20 01:18:50.93|\n|66       |2025-01-20 01:18:51.445|66         |2025-01-20 01:19:01.73|\n|13       |2025-01-20 01:18:40.845|13         |2025-01-20 01:18:51.13|\n|36       |2025-01-20 01:18:45.445|36         |2025-01-20 01:18:55.73|\n|14       |2025-01-20 01:18:41.045|14         |2025-01-20 01:18:51.33|\n|38       |2025-01-20 01:18:45.845|38         |2025-01-20 01:18:56.13|\n|53       |2025-01-20 01:18:48.845|53         |2025-01-20 01:18:59.13|\n|188      |2025-01-20 01:19:15.845|188        |2025-01-20 01:19:26.13|\n|136      |2025-01-20 01:19:05.445|136        |2025-01-20 01:19:15.73|\n|181      |2025-01-20 01:19:14.445|181        |2025-01-20 01:19:24.73|\n|146      |2025-01-20 01:19:07.445|146        |2025-01-20 01:19:17.73|\n|174      |2025-01-20 01:19:13.045|174        |2025-01-20 01:19:23.33|\n|111      |2025-01-20 01:19:00.445|111        |2025-01-20 01:19:10.73|\n|186      |2025-01-20 01:19:15.445|186        |2025-01-20 01:19:25.73|\n|171      |2025-01-20 01:19:12.445|171        |2025-01-20 01:19:22.73|\n|154      |2025-01-20 01:19:09.045|154        |2025-01-20 01:19:19.33|\n|134      |2025-01-20 01:19:05.045|134        |2025-01-20 01:19:15.33|\n|194      |2025-01-20 01:19:17.045|194        |2025-01-20 01:19:27.33|\n|121      |2025-01-20 01:19:02.445|121        |2025-01-20 01:19:12.73|\n+---------+-----------------------+-----------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "non_empty_count = 0\n",
    "while non_empty_count < 3:\n",
    "    result = spark.sql(\"SELECT * FROM inner_join_impression_click\")\n",
    "    row_count = result.count()\n",
    "    if row_count == 0:\n",
    "        continue\n",
    "    non_empty_count += 1\n",
    "    result.show(row_count, truncate=False)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4281079-8a48-4cbb-be74-f97be58e287a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    inner_join_query.stop()\n",
    "    inner_join_query.awaitTermination()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb443f2-03b7-41d3-b02a-794faf264ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part 1b\n",
    "Inspect the output of your streaming query. For the moment, please ignore the warning messages that are outputted in the cell output. Instead, examine the outputs of the SELECT statements. What do you notice about the tables that are outputted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96a6768-2abf-4950-b5b6-c0ab31a7f5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Answer to Part 1b**: \n",
    "The streaming query shows how new data keeps getting added over time, while older data stays unchanged. Each new set of rows represents recent events, and the timestamps show the data is processed in order. The IDs match between ad impressions and clicks, showing a clear link between the two. This is a simple example of how real-time data is handled, adding new information while keeping past data intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e69ec3-ec0c-4940-a36a-72c7ca93be54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Part 2: Adding More Logic to Inner Join\n",
    "### Part 2a\n",
    "Note that a plain inner join will happily join an impression with a click that could be far away. Normally in computational advertisement, we would want to constrain the impression/click join within a minute or two. This is especially true in a mobile device. \n",
    "\n",
    "Write a code that enforces that a click can occur within a time range of 0 seconds to 1 minute after the corresponding impression. Specify this as a join condition between `impressionTime` and `clickTime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "743cbcc6-5bda-4a29-8ab4-80e468f21995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Complete this part for Part 2a.\n",
    "from pyspark.sql.functions import expr\n",
    "restricted_joined_stream_name = 'rest_inner_join_impression_click'\n",
    "\n",
    "rest_inner_join_query = impressions.join(clicks, expr(\"shownAdId = clickedAdId AND clickTime > impressionTime \"))\\\n",
    "  .writeStream \\\n",
    "    .outputMode('append')\\\n",
    "      .format('memory')\\\n",
    "        .queryName(restricted_joined_stream_name)\\\n",
    "          .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b448c4-7f21-4e62-bcf4-6cbb3ffe2d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+-----------+-----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime              |\n+---------+-----------------------+-----------+-----------------------+\n|19       |2025-01-20 01:36:19.606|19         |2025-01-20 01:36:29.956|\n|95       |2025-01-20 01:36:34.806|95         |2025-01-20 01:36:45.156|\n|6        |2025-01-20 01:36:17.006|6          |2025-01-20 01:36:27.356|\n|88       |2025-01-20 01:36:33.406|88         |2025-01-20 01:36:43.756|\n|1        |2025-01-20 01:36:16.006|1          |2025-01-20 01:36:26.356|\n|69       |2025-01-20 01:36:29.606|69         |2025-01-20 01:36:39.956|\n|59       |2025-01-20 01:36:27.606|59         |2025-01-20 01:36:37.956|\n|47       |2025-01-20 01:36:25.206|47         |2025-01-20 01:36:35.556|\n+---------+-----------------------+-----------+-----------------------+\n\n+---------+-----------------------+-----------+-----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime              |\n+---------+-----------------------+-----------+-----------------------+\n|19       |2025-01-20 01:36:19.606|19         |2025-01-20 01:36:29.956|\n|95       |2025-01-20 01:36:34.806|95         |2025-01-20 01:36:45.156|\n|6        |2025-01-20 01:36:17.006|6          |2025-01-20 01:36:27.356|\n|88       |2025-01-20 01:36:33.406|88         |2025-01-20 01:36:43.756|\n|1        |2025-01-20 01:36:16.006|1          |2025-01-20 01:36:26.356|\n|69       |2025-01-20 01:36:29.606|69         |2025-01-20 01:36:39.956|\n|59       |2025-01-20 01:36:27.606|59         |2025-01-20 01:36:37.956|\n|47       |2025-01-20 01:36:25.206|47         |2025-01-20 01:36:35.556|\n+---------+-----------------------+-----------+-----------------------+\n\n+---------+-----------------------+-----------+-----------------------+\n|shownAdId|impressionTime         |clickedAdId|clickTime              |\n+---------+-----------------------+-----------+-----------------------+\n|19       |2025-01-20 01:36:19.606|19         |2025-01-20 01:36:29.956|\n|95       |2025-01-20 01:36:34.806|95         |2025-01-20 01:36:45.156|\n|6        |2025-01-20 01:36:17.006|6          |2025-01-20 01:36:27.356|\n|88       |2025-01-20 01:36:33.406|88         |2025-01-20 01:36:43.756|\n|1        |2025-01-20 01:36:16.006|1          |2025-01-20 01:36:26.356|\n|69       |2025-01-20 01:36:29.606|69         |2025-01-20 01:36:39.956|\n|59       |2025-01-20 01:36:27.606|59         |2025-01-20 01:36:37.956|\n|47       |2025-01-20 01:36:25.206|47         |2025-01-20 01:36:35.556|\n|136      |2025-01-20 01:36:43.006|136        |2025-01-20 01:36:53.356|\n|145      |2025-01-20 01:36:44.806|145        |2025-01-20 01:36:55.156|\n|124      |2025-01-20 01:36:40.606|124        |2025-01-20 01:36:50.956|\n|174      |2025-01-20 01:36:50.606|174        |2025-01-20 01:37:00.956|\n|120      |2025-01-20 01:36:39.806|120        |2025-01-20 01:36:50.156|\n|111      |2025-01-20 01:36:38.006|111        |2025-01-20 01:36:48.356|\n|108      |2025-01-20 01:36:37.406|108        |2025-01-20 01:36:47.756|\n|132      |2025-01-20 01:36:42.206|132        |2025-01-20 01:36:52.556|\n|123      |2025-01-20 01:36:40.406|123        |2025-01-20 01:36:50.756|\n|160      |2025-01-20 01:36:47.806|160        |2025-01-20 01:36:58.156|\n|101      |2025-01-20 01:36:36.006|101        |2025-01-20 01:36:46.356|\n|186      |2025-01-20 01:36:53.006|186        |2025-01-20 01:37:03.356|\n|171      |2025-01-20 01:36:50.006|171        |2025-01-20 01:37:00.356|\n|118      |2025-01-20 01:36:39.406|118        |2025-01-20 01:36:49.756|\n|129      |2025-01-20 01:36:41.606|129        |2025-01-20 01:36:51.956|\n|195      |2025-01-20 01:36:54.806|195        |2025-01-20 01:37:05.156|\n+---------+-----------------------+-----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "non_empty_count = 0\n",
    "while non_empty_count < 3:\n",
    "    result = spark.sql(\"SELECT * FROM rest_inner_join_impression_click\")\n",
    "    row_count = result.count()\n",
    "    if row_count == 0:\n",
    "        continue\n",
    "    non_empty_count += 1\n",
    "    result.show(row_count, truncate=False)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89aa5c47-1805-4857-b1d6-a86f3a27bf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    rest_inner_join_query.stop()\n",
    "    rest_inner_join_query.awaitTermination()\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ds610_week_08_hw_starter_code",
   "widgets": {}
  },
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}